{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Crawler\n",
    "\n",
    "## 1.0. Related example\n",
    "\n",
    "This code shows `wget`-like tool written in python. Run it from console (`python wget.py`), make it work. Check the code, reuse, and modify for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "\n",
    "def wget(url, filename):\n",
    "    # allow redirects - in case file is relocated\n",
    "    resp = requests.get(url, allow_redirects=True)\n",
    "    \n",
    "    # this can also be 2xx, but for simplicity now we stick to 200\n",
    "    # you can also check for `resp.ok`\n",
    "    if resp.status_code != 200:\n",
    "        print(resp.status_code, resp.reason, 'for', url)\n",
    "        return\n",
    "    \n",
    "    # just to be cool and print something\n",
    "    print(*[f\"{key}: {value}\" for key, value in resp.headers.items()], sep='\\n')\n",
    "    print()\n",
    "    \n",
    "    # try to extract filename from url\n",
    "    if filename is None:\n",
    "        # start with http*, ends if ? or # appears (or none of)\n",
    "        m = re.search(\"^http.*/([^/\\?#]*)[\\?#]?\", url)\n",
    "        filename = m.group(1)\n",
    "        if not filename:\n",
    "            raise NameError(f\"Filename neither given, nor found for {url}\")\n",
    "\n",
    "    # what will you do in case 2 websites store file with the same name?\n",
    "    # the real wget will just download again with \"filename.ext.<frequency>\"\n",
    "    if os.path.exists(filename):\n",
    "        raise OSError(f\"File {filename} already exists\")\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(resp.content)\n",
    "        print(f\"File saved as {filename}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description='download file.')\n",
    "#     parser.add_argument(\"-O\", type=str, default=None, dest='filename', help=\"output file name. Default -- taken from resource\")\n",
    "#     parser.add_argument(\"url\", type=str, default=None, help=\"Provide URL here\")\n",
    "#     args = parser.parse_args()\n",
    "#     wget(args.url, args.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0.1. How to parse a page?\n",
    "\n",
    "If you build a crawler, you might follow one of the approaches:\n",
    "1. search for URLs in the page, assuming this is just a text.\n",
    "2. search for URLs in the places where URLs should appear: `<a href=..`, `<img src=...`, `<iframe src=...` and so on.\n",
    "\n",
    "To follow the first approach you can rely on some good regular expression. [Like this](https://stackoverflow.com/a/3809435).\n",
    "\n",
    "To follow the second approach just read one of these: [short answer](https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup) or [exhaustive explanation](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. [15] Download and persist #\n",
    "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
    "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
    "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
    "- `load()` method loads data from hard drive. Returns `True` for success.\n",
    "\n",
    "Tests checks that your code somehow works.\n",
    "\n",
    "**NB Passing the test doesn't mean you correctly completed the task.** These are **criteria, which have to be fullfilled**:\n",
    "1. URL is a unique identifier (as it is a subset of URI). Thus, documents with different URLs should be stored in different files. Typical errors: documents from the same domain are overwritten to the same file, URLs with similar endings are downloaded to the same file, etc.\n",
    "2. The document can be not only a text file, but also a binary. Pay attention that if you download `mp3` file, it still can be played. Hint: don't hurry to convert everything to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('tmp'):\n",
    "    os.mkdir('tmp') # for storing webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class Document:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        if url[-1] == '/': # remove trailing slash from url\n",
    "            url = url[:-1]\n",
    "        self.url = url\n",
    "        self.filename = None\n",
    "        \n",
    "    def get(self): # load file if cached, download and persist otherwise\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "    \n",
    "    def download(self):\n",
    "        r = requests.get(self.url, allow_redirects=True)\n",
    "        if r.status_code == 200:\n",
    "            self.content = r.content\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def persist(self):\n",
    "        m = re.search(\"^http.*/([^/\\?#]*)[\\?#]?\", self.url)\n",
    "        self.filename = m.group(1)\n",
    "        \n",
    "        if self.filename == '':\n",
    "            self.filename = 'index.html'\n",
    "            \n",
    "        with open('tmp/' + self.filename, 'wb') as f:\n",
    "            f.write(self.content)\n",
    "            print(f\"File saved as tmp/{self.filename}\")\n",
    "            \n",
    "    def load(self):\n",
    "        if not self.filename:  # file not cached\n",
    "            return False\n",
    "            \n",
    "        with open('tmp/' + self.filename, 'rb') as f:\n",
    "            self.content = f.read()\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as tmp/iu.txt\n"
     ]
    }
   ],
   "source": [
    "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
    "\n",
    "doc.get()\n",
    "assert doc.content, \"Document download failed\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
    "\n",
    "doc.get()\n",
    "assert doc.load(), \"Load should return true for saved document\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. [M][15] Account the caching policy\n",
    "\n",
    "Sometimes remote documents (especially when we speak about static content like `js` or `gif`) can swear that they will not change for some time. This is done by setting [Cache-Control response header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'public, s-maxage=31536000, max-age=604800, stale-while-revalidate=604800, stale-if-error=604800'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get('https://polyfill.io/v3/polyfill.min.js').headers['Cache-Control']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please study the documentation and implement a descendant to a `Document` class, which will refresh the document in case of expired cache even if the file is already on the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class CachedDocument(Document):\n",
    "    def load(self):\n",
    "        # if the file is not cached, or max-age expired, return False so that a new copy is requested.\n",
    "        max_age = re.search('max-age=(\\d+)', requests.get(self.url).headers['Cache-Control'])\n",
    "        if not self.filename or time.time() - self.ts > int(max_age.group(1)):\n",
    "            self.ts = time.time()\n",
    "            return False\n",
    "        \n",
    "        print(f'{self.filename} is cached and will not be requested again.')\n",
    "        with open('tmp/' + self.filename, 'rb') as f:\n",
    "            self.content = f.read()\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Tests\n",
    "\n",
    "Add logging in your code and show that your code behaves differently for documents with different caching policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as tmp/polyfill.min.js\n",
      "polyfill.min.js is cached and will not be requested again.\n",
      "polyfill.min.js is cached and will not be requested again.\n",
      "File saved as tmp/google.com\n",
      "File saved as tmp/google.com\n",
      "File saved as tmp/google.com\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "doc = CachedDocument('https://polyfill.io/v3/polyfill.min.js')\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()\n",
    "\n",
    "doc = CachedDocument('https://google.com/') # beware bot detectors :)\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. [10] Parse HTML ##\n",
    "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
    "- `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
    "- `self.images` list of images met in a document. Again, links can be relative to current page.\n",
    "- `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details.\n",
    "\n",
    "**NB All these 3 criteria must be fulfilled to get full point for the task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from bs4.element import Comment\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "    \n",
    "    def _tag_visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _extract_text(self):\n",
    "        soup = BeautifulSoup(self.content, 'html.parser')\n",
    "        texts = soup.findAll(text=True)\n",
    "        visible_texts = filter(self._tag_visible, texts)  \n",
    "        return u\" \".join(t.strip() for t in visible_texts)\n",
    "    \n",
    "    def _extract_images(self):\n",
    "        images = []\n",
    "            \n",
    "        for link in BeautifulSoup(self.content, 'html.parser', parse_only=SoupStrainer('img')):\n",
    "            if link.has_attr('src'):\n",
    "                if not link['src'].startswith('http'): # relative link\n",
    "                    images.append(urljoin(self.url, link['src']))\n",
    "                else:\n",
    "                    images.append(link['src'])\n",
    "        return images\n",
    "    \n",
    "    def _extract_anchors(self):\n",
    "        anchors = []\n",
    "            \n",
    "        for link in BeautifulSoup(self.content, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "            if link.has_attr('href'):\n",
    "                link.contents.append('')\n",
    "                if not link['href'].startswith('http'): # relative link\n",
    "                    anchors.append((link.contents[0], urljoin(self.url, link['href'])))\n",
    "                else:\n",
    "                    anchors.append((link.contents[0], link['href']))\n",
    "        return anchors\n",
    "    \n",
    "    def parse(self):\n",
    "        self.text = self._extract_text()\n",
    "        self.anchors = self._extract_anchors()\n",
    "        self.images = self._extract_images()\n",
    "        #print(self.text, self.anchors, self.images, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as tmp/sprotasov.ru\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "assert \"just few links\" in doc.text, \"Error parsing text\"\n",
    "assert \"http://sprotasov.ru/images/gb.svg\" in doc.images, \"Error parsing images\"\n",
    "assert any(p[1] == \"https://twitter.com/07C3\" for p in doc.anchors), \"Error parsing links\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. [10] Document analysis ##\n",
    "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose). \n",
    "\n",
    "**Criteria of success**: \n",
    "1. Your `get_word_stats()` method should return `Counter` object.\n",
    "2. Don't forget to lowercase your words for counting.\n",
    "3. Sentences should be obtained inside `<body>` tag only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 646 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /home/ahmed/.local/lib/python3.8/site-packages (from nltk) (8.0.3)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.1.18-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[K     |████████████████████████████████| 764 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib, tqdm, regex, nltk\n",
      "Successfully installed joblib-1.1.0 nltk-3.6.7 regex-2022.1.18 tqdm-4.62.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ahmed/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/ahmed/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class HtmlDocumentTextData:  # This class assumes english documents, multilingual version below\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.doc = HtmlDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        return nltk.tokenize.sent_tokenize(self.doc.text)\n",
    "    \n",
    "    def get_word_stats(self):\n",
    "        wc = {}\n",
    "        words = [w for w in nltk.tokenize.word_tokenize(self.doc.text) if (not w in stopwords.words('english')) and w.isalpha()]\n",
    "        for word in words:\n",
    "            wc[word.lower()] = wc.get(word.lower(), 0) + 1\n",
    "        return Counter(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as tmp/innopolis.university\n",
      "[('и', 59), ('в', 30), ('иннополис', 20), ('по', 17), ('на', 14), ('университет', 12), ('области', 10), ('с', 10), ('лаборатория', 10), ('университета', 9)]\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
    "\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис should be among most common'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. [M][35] Languages\n",
    "Maybe you heard, that there are multiple languages in the world. European languages, like Russian and English, use similar puctuation, but even in this family there is ¡Spanish!\n",
    "\n",
    "Other languages can use different punctiation rules, like **Arabic or [Thai](http://www.thai-language.com/ref/breaking-words)**.\n",
    "\n",
    "Your task is to support (at least) three languages (English, Arabic, and Thai) tokenization in your `HtmlDocumentTextData` class descendant.\n",
    "\n",
    "What should you do:\n",
    "1. Use any language dection techniques, e.g. [langdetect](https://pypi.org/project/langdetect/).\n",
    "2. Use language-specific tokenization tools, e.g. for [Thai](https://pythainlp.github.io/tutorials/notebooks/pythainlp_get_started.html#Tokenization-and-Segmentation) and [Arabic](https://github.com/CAMeL-Lab/camel_tools).\n",
    "3. Use these pages to test your code: [1](https://www.bangkokair.com/tha/baggage-allowance) and [2](https://alfajr-news.net/details/%D9%85%D8%B4%D8%B1%D9%88%D8%B9-%D8%AF%D9%8A%D9%85%D9%88%D9%82%D8%B1%D8%A7%D8%B7%D9%8A-%D9%81%D9%8A-%D8%A7%D9%84%D9%83%D9%88%D9%86%D8%BA%D8%B1%D8%B3-%D8%A7%D9%84%D8%A3%D9%85%D8%B1%D9%8A%D9%83%D9%8A-%D9%84%D9%85%D8%B9%D8%A7%D9%82%D8%A8%D8%A9-%D8%A8%D9%88%D8%AA%D9%8A%D9%86)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from langdetect) (1.14.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=388ea8c5a8619a826cf69749780578f3a94d6db773439e098894e99e00641ca6\n",
      "  Stored in directory: /home/ahmed/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting iso-639\n",
      "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
      "\u001b[K     |████████████████████████████████| 167 kB 502 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: iso-639\n",
      "  Building wheel for iso-639 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=b507358a18ddcdf304081b5ebd7f84c925e77e34b59d63f468b9bb3ba48184a7\n",
      "  Stored in directory: /home/ahmed/.cache/pip/wheels/ed/ce/cc/1961a4de7090b2e92895fb087abfa0080a542a5706c5948bcc\n",
      "Successfully built iso-639\n",
      "Installing collected packages: iso-639\n",
      "Successfully installed iso-639-0.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install iso-639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stopwordsiso\n",
      "  Downloading stopwordsiso-0.6.1-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: stopwordsiso\n",
      "Successfully installed stopwordsiso-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install stopwordsiso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import stopwordsiso\n",
    "from iso639 import languages\n",
    "import string\n",
    "\n",
    "class MultilingualHtmlDocumentTextData(HtmlDocumentTextData):\n",
    "    def _get_document_lang(self):\n",
    "        lang = languages.get(alpha2=detect(self.doc.text)).name.lower()\n",
    "        return lang\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        try:  # use dedicated language tokenizer if available, use default otherwise.\n",
    "            return nltk.tokenize.sent_tokenize(self.doc.text, language=self._get_document_lang())\n",
    "        except LookupError:\n",
    "            return nltk.tokenize.sent_tokenize(self.doc.text)\n",
    "        \n",
    "    def get_word_stats(self):\n",
    "        lang = detect(self.doc.text)\n",
    "        if stopwordsiso.has_lang(lang):\n",
    "            sw = stopwordsiso.stopwords(lang)\n",
    "        else:\n",
    "            sw = []\n",
    "        words = [w for w in nltk.tokenize.word_tokenize(self.doc.text) if (not w in sw) and w.isalpha()]\n",
    "        wc = {}\n",
    "        for word in words:\n",
    "            wc[word.lower()] = wc.get(word.lower(), 0) + 1\n",
    "        return Counter(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as tmp/baggage-allowance\n",
      "[('โซน', 11), ('x', 6), ('ภาษาไทย', 4), ('usd', 4), ('english', 2), ('繁體中文', 2), ('简体中文', 2), ('thb', 2), ('sgd', 2), ('myr', 2)]\n",
      "File saved as tmp/%D9%85%D8%B4%D8%B1%D9%88%D8%B9-%D8%AF%D9%8A%D9%85%D9%88%D9%82%D8%B1%D8%A7%D8%B7%D9%8A-%D9%81%D9%8A-%D8%A7%D9%84%D9%83%D9%88%D9%86%D8%BA%D8%B1%D8%B3-%D8%A7%D9%84%D8%A3%D9%85%D8%B1%D9%8A%D9%83%D9%8A-%D9%84%D9%85%D8%B9%D8%A7%D9%82\n",
      "[('تعليق', 14), ('مشاهده', 11), ('الإمارات', 5), ('الفجر', 4), ('فن', 4), ('محمد', 3), ('دبي', 3), ('أخبار', 3), ('صحة', 3), ('أغسطس', 3)]\n"
     ]
    }
   ],
   "source": [
    "doc = MultilingualHtmlDocumentTextData(\"https://www.bangkokair.com/tha/baggage-allowance\")\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "\n",
    "doc = MultilingualHtmlDocumentTextData(\"https://alfajr-news.net/details/%D9%85%D8%B4%D8%B1%D9%88%D8%B9-%D8%AF%D9%8A%D9%85%D9%88%D9%82%D8%B1%D8%A7%D8%B7%D9%8A-%D9%81%D9%8A-%D8%A7%D9%84%D9%83%D9%88%D9%86%D8%BA%D8%B1%D8%B3-%D8%A7%D9%84%D8%A3%D9%85%D8%B1%D9%8A%D9%83%D9%8A-%D9%84%D9%85%D8%B9%D8%A7%D9%82\")\n",
    "print(doc.get_word_stats().most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. [15] Crawling ##\n",
    "\n",
    "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self):\n",
    "        self.visited = {}\n",
    "    \n",
    "    def remove_trailing_slash(self, url):\n",
    "        if url[-1] == '/': # remove trailing slash from url\n",
    "            url = url[:-1]\n",
    "        return url\n",
    "        \n",
    "    def crawl_generator(self, source, depth=0):\n",
    "        links = Queue()\n",
    "        \n",
    "        source = self.remove_trailing_slash(source)\n",
    "        links.put((source, 0))\n",
    "        \n",
    "        while not links.empty():\n",
    "            url, dep = links.get()\n",
    "            url = self.remove_trailing_slash(url)\n",
    "            self.visited[url] = True\n",
    "            try:\n",
    "                page = HtmlDocumentTextData(url)\n",
    "                yield page\n",
    "            \n",
    "            except:\n",
    "                print(f'[WARN] Failed to scrape {url}')\n",
    "                continue\n",
    "                \n",
    "            if dep == depth: # max depth reached\n",
    "                return\n",
    "            \n",
    "            for _, link in page.doc.anchors:\n",
    "                link = self.remove_trailing_slash(link)\n",
    "                if not self.visited.get(link):\n",
    "                    links.put((link, dep + 1))\n",
    "                    self.visited[link] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as tmp/en\n",
      "https://innopolis.university/en\n",
      "281 distinct word(s) so far\n",
      "File saved as tmp/en\n",
      "https://apply.innopolis.university/en\n",
      "906 distinct word(s) so far\n",
      "File saved as tmp/en\n",
      "https://corporate.innopolis.university/en\n",
      "1050 distinct word(s) so far\n",
      "File saved as tmp/en\n",
      "https://media.innopolis.university/en\n",
      "1092 distinct word(s) so far\n",
      "File saved as tmp/lk\n",
      "https://innopolis.university/lk\n",
      "1436 distinct word(s) so far\n",
      "File saved as tmp/about\n",
      "https://innopolis.university/en/about\n",
      "1533 distinct word(s) so far\n",
      "File saved as tmp/board\n",
      "https://innopolis.university/en/board\n",
      "1605 distinct word(s) so far\n",
      "File saved as tmp/team\n",
      "https://innopolis.university/en/team\n",
      "1606 distinct word(s) so far\n",
      "File saved as tmp/team-structure\n",
      "https://innopolis.university/en/team-structure\n",
      "1609 distinct word(s) so far\n",
      "File saved as tmp/education-academics\n",
      "https://innopolis.university/en/team-structure/education-academics\n",
      "1613 distinct word(s) so far\n",
      "File saved as tmp/techcenters\n",
      "https://innopolis.university/en/team-structure/techcenters\n",
      "1615 distinct word(s) so far\n",
      "File saved as tmp/faculty\n",
      "https://innopolis.university/en/faculty\n",
      "2384 distinct word(s) so far\n",
      "File saved as tmp/job\n",
      "https://career.innopolis.university/en/job\n",
      "2773 distinct word(s) so far\n",
      "File saved as tmp/en\n",
      "https://career.innopolis.university/en\n",
      "3006 distinct word(s) so far\n",
      "File saved as tmp/campus\n",
      "https://innopolis.university/en/campus\n",
      "3102 distinct word(s) so far\n",
      "File saved as tmp/contacts\n",
      "https://innopolis.university/en/contacts\n",
      "3104 distinct word(s) so far\n",
      "File saved as tmp/bachelor\n",
      "https://apply.innopolis.university/en/bachelor\n",
      "3152 distinct word(s) so far\n",
      "File saved as tmp/CE\n",
      "https://apply.innopolis.university/en/bachelor/CE\n",
      "3163 distinct word(s) so far\n",
      "File saved as tmp/DS-AI\n",
      "https://apply.innopolis.university/en/bachelor/DS-AI\n",
      "3179 distinct word(s) so far\n",
      "File saved as tmp/master\n",
      "https://apply.innopolis.university/en/master\n",
      "3194 distinct word(s) so far\n",
      "File saved as tmp/datascience\n",
      "https://apply.innopolis.university/en/master/datascience\n",
      "3252 distinct word(s) so far\n",
      "File saved as tmp/securityandnetworkengineering\n",
      "https://apply.innopolis.university/en/master/securityandnetworkengineering\n",
      "3291 distinct word(s) so far\n",
      "File saved as tmp/development\n",
      "https://apply.innopolis.university/en/master/development\n",
      "3331 distinct word(s) so far\n",
      "File saved as tmp/robotics\n",
      "https://apply.innopolis.university/en/master/robotics\n",
      "3379 distinct word(s) so far\n",
      "File saved as tmp/technological-entrepreneurship\n",
      "https://apply.innopolis.university/en/master/technological-entrepreneurship\n",
      "3511 distinct word(s) so far\n",
      "File saved as tmp/postgraduate-study\n",
      "https://apply.innopolis.university/en/postgraduate-study\n",
      "3570 distinct word(s) so far\n",
      "File saved as tmp/stud-life\n",
      "https://apply.innopolis.university/en/stud-life\n",
      "3623 distinct word(s) so far\n",
      "File saved as tmp/international-relations-office\n",
      "https://innopolis.university/en/international-relations-office\n",
      "3845 distinct word(s) so far\n",
      "File saved as tmp/incomingstudents\n",
      "https://innopolis.university/en/incomingstudents\n",
      "3876 distinct word(s) so far\n",
      "File saved as tmp/outgoingstudents\n",
      "https://innopolis.university/en/outgoingstudents\n",
      "3895 distinct word(s) so far\n",
      "File saved as tmp/teachingexcellencecenter\n",
      "https://innopolis.university/en/teachingexcellencecenter\n",
      "4079 distinct word(s) so far\n",
      "File saved as tmp/writinghubhome\n",
      "https://innopolis.university/en/writinghubhome\n",
      "4091 distinct word(s) so far\n",
      "File saved as tmp/alumni.innopolis.university\n",
      "https://alumni.innopolis.university\n",
      "4287 distinct word(s) so far\n",
      "File saved as tmp/research\n",
      "https://innopolis.university/en/research\n",
      "4309 distinct word(s) so far\n",
      "File saved as tmp/lab-operating-systems\n",
      "https://innopolis.university/en/lab-operating-systems\n",
      "4330 distinct word(s) so far\n",
      "File saved as tmp/lab-software-service-engineering\n",
      "https://innopolis.university/en/lab-software-service-engineering\n",
      "4413 distinct word(s) so far\n",
      "File saved as tmp/lab-industrializing-software\n",
      "https://innopolis.university/en/lab-industrializing-software\n",
      "4451 distinct word(s) so far\n",
      "File saved as tmp/lab-bioinformatics\n",
      "https://innopolis.university/en/lab-bioinformatics\n",
      "4482 distinct word(s) so far\n",
      "File saved as tmp/lab-game-development\n",
      "https://innopolis.university/en/lab-game-development\n",
      "4525 distinct word(s) so far\n",
      "File saved as tmp/lab-oil-gas\n",
      "https://innopolis.university/en/lab-oil-gas\n",
      "4560 distinct word(s) so far\n",
      "File saved as tmp/mlkr\n",
      "https://innopolis.university/en/mlkr\n",
      "4653 distinct word(s) so far\n",
      "File saved as tmp/lab-cyberphysical-systems\n",
      "https://innopolis.university/en/lab-cyberphysical-systems\n",
      "4678 distinct word(s) so far\n",
      "File saved as tmp/lab-networks-blockchain\n",
      "https://innopolis.university/en/lab-networks-blockchain\n",
      "4717 distinct word(s) so far\n",
      "File saved as tmp/lab-robotics\n",
      "https://innopolis.university/en/lab-robotics\n",
      "4785 distinct word(s) so far\n",
      "File saved as tmp/activity\n",
      "https://innopolis.university/en/proekty/activity\n",
      "4807 distinct word(s) so far\n",
      "File saved as tmp/podderzhka-innovacionnoj-deyatelnosti\n",
      "https://innopolis.university/en/proekty/podderzhka-innovacionnoj-deyatelnosti\n",
      "4837 distinct word(s) so far\n",
      "File saved as tmp/startupstudio\n",
      "https://innopolis.university/en/startupstudio\n",
      "4861 distinct word(s) so far\n",
      "File saved as tmp/internationalpartners\n",
      "https://innopolis.university/en/internationalpartners\n",
      "4863 distinct word(s) so far\n",
      "File saved as tmp/organizatsiya-i-provedenie-meropriyatiy\n",
      "https://innopolis.university/en/organizatsiya-i-provedenie-meropriyatiy\n",
      "4948 distinct word(s) so far\n",
      "File saved as tmp/index.html\n",
      "https://innopolis.university/en/?special=Y\n",
      "4958 distinct word(s) so far\n",
      "File saved as tmp/search\n",
      "https://innopolis.university/search\n",
      "4961 distinct word(s) so far\n",
      "File saved as tmp/innopolis.university\n",
      "https://innopolis.university\n",
      "5115 distinct word(s) so far\n",
      "File saved as tmp/ido\n",
      "https://innopolis.university/en/ido\n",
      "5131 distinct word(s) so far\n",
      "File saved as tmp/dovuz.innopolis.university\n",
      "https://dovuz.innopolis.university\n",
      "5445 distinct word(s) so far\n",
      "File saved as tmp/about\n",
      "https://university.innopolis.ru/en/about\n",
      "5445 distinct word(s) so far\n",
      "File saved as tmp/www.campuslife.innopolis.ru\n",
      "http://www.campuslife.innopolis.ru\n",
      "5513 distinct word(s) so far\n",
      "File saved as tmp/clobal-ai-challenge\n",
      "https://media.innopolis.university/news/clobal-ai-challenge\n",
      "5600 distinct word(s) so far\n",
      "File saved as tmp/webinar-interstudents-eng\n",
      "https://media.innopolis.university/news/webinar-interstudents-eng\n",
      "5600 distinct word(s) so far\n",
      "File saved as tmp/devops-summer-school\n",
      "https://media.innopolis.university/news/devops-summer-school\n",
      "5600 distinct word(s) so far\n",
      "File saved as tmp/webinar-for-international-candidates-\n",
      "https://media.innopolis.university/news/webinar-for-international-candidates-\n",
      "5600 distinct word(s) so far\n",
      "File saved as tmp/registration-innopolis-open-2020\n",
      "https://media.innopolis.university/news/registration-innopolis-open-2020\n",
      "5600 distinct word(s) so far\n",
      "File saved as tmp/cyber-resilience-petrenko\n",
      "https://media.innopolis.university/news/cyber-resilience-petrenko\n",
      "5600 distinct word(s) so far\n",
      "File saved as tmp/innopolis-university-extends-international-application-deadline-\n",
      "https://media.innopolis.university/news/innopolis-university-extends-international-application-deadline-\n",
      "5600 distinct word(s) so far\n",
      "File saved as tmp/InnopolisU\n",
      "https://www.facebook.com/InnopolisU\n",
      "5600 distinct word(s) so far\n",
      "File saved as tmp/innopolisu\n",
      "https://vk.com/innopolisu\n",
      "5763 distinct word(s) so far\n",
      "File saved as tmp/InnopolisU\n",
      "https://www.youtube.com/user/InnopolisU\n",
      "5774 distinct word(s) so far\n",
      "File saved as tmp/innopolisu\n",
      "https://www.instagram.com/innopolisu\n",
      "5774 distinct word(s) so far\n",
      "File saved as tmp/index.php\n",
      "https://apply.innopolis.ru/en/index.php\n",
      "6032 distinct word(s) so far\n",
      "[WARN] Failed to scrape https://university.innopolis.ru/en/cooperation\n",
      "File saved as tmp/NvQZM6B2\n",
      "https://panoroo.com/virtual-tours/NvQZM6B2\n",
      "6032 distinct word(s) so far\n",
      "File saved as tmp/news\n",
      "https://media.innopolis.university/en/news\n",
      "6032 distinct word(s) so far\n",
      "File saved as tmp/events\n",
      "https://media.innopolis.university/en/events\n",
      "6034 distinct word(s) so far\n",
      "[WARN] Failed to scrape http://www.minsvyaz.ru/en\n",
      "[WARN] Failed to scrape http://минобрнауки.рф\n",
      "File saved as tmp/Consent_to_the_processing_of_PD_for_UI.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
      "Skipping https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
      "File saved as tmp/index.html\n",
      "https://apply.innopolis.university/en/?special=Y\n",
      "6034 distinct word(s) so far\n",
      "Done\n",
      "[('university', 1131), ('innopolis', 582), ('research', 529), ('development', 521), ('lab', 517), ('education', 484), ('и', 483), ('science', 475), ('students', 467), ('software', 443), ('data', 431), ('robotics', 377), ('it', 355), ('the', 345), ('в', 340), ('engineering', 334), ('systems', 328), ('intelligence', 305), ('artificial', 302), ('computer', 300)]\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler()\n",
    "counter = Counter()\n",
    "\n",
    "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 2):\n",
    "    print(c.doc.url)\n",
    "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "        print(\"Skipping\", c.doc.url)\n",
    "        continue\n",
    "    counter.update(c.get_word_stats())\n",
    "    print(len(counter), \"distinct word(s) so far\")\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "print(counter.most_common(20))\n",
    "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis should be among most common'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
